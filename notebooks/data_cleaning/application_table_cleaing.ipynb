{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5124850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory for notebook:  /workspace\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "CURRENT_DIRECTORY_NOTEBOOK = None\n",
    "\n",
    "\n",
    "def intitate_notebook():\n",
    "    load_dotenv()\n",
    "    global CURRENT_DIRECTORY_NOTEBOOK\n",
    "    if CURRENT_DIRECTORY_NOTEBOOK is None:\n",
    "        os.chdir(os.getenv(\"PROJECT_BASE_PATH\"))\n",
    "        CURRENT_DIRECTORY_NOTEBOOK = Path(os.getcwd())\n",
    "        print(\"Current directory for notebook: \", CURRENT_DIRECTORY_NOTEBOOK)\n",
    "    else:\n",
    "        print(\n",
    "            \"Current directory for notebook is already set: \",\n",
    "            CURRENT_DIRECTORY_NOTEBOOK,\n",
    "        )\n",
    "\n",
    "\n",
    "intitate_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a296f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from src.data.explore_column_info import get_column_summary\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e05eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/03 10:10:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/03 10:10:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"PostgresETL\")\n",
    "    .config(\"spark.jars\", \"setup_files/postgresql-42.7.5.jar\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c329ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = \"data_source_user\"\n",
    "password = \"data_source_user_password\"\n",
    "host = \"172.17.0.1\"\n",
    "port = \"5435\"\n",
    "database = \"data_source_db\"\n",
    "\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{host}:{port}/{database}\"\n",
    "properties = {\"user\": username, \"password\": password, \"driver\": \"org.postgresql.Driver\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48d954c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[main_split_id: bigint, sk_id_curr: bigint, target: bigint, name_contract_type: string, code_gender: string, flag_own_car: string, flag_own_realty: string, cnt_children: bigint, amt_income_total: double, amt_credit: double, amt_annuity: double, amt_goods_price: double, name_type_suite: string, name_income_type: string, name_education_type: string, name_family_status: string, name_housing_type: string, region_population_relative: double, days_birth: bigint, days_employed: bigint, days_registration: double, days_id_publish: bigint, own_car_age: double, flag_mobil: bigint, flag_emp_phone: bigint, flag_work_phone: bigint, flag_cont_mobile: bigint, flag_phone: bigint, flag_email: bigint, occupation_type: string, cnt_fam_members: double, region_rating_client: bigint, region_rating_client_w_city: bigint, weekday_appr_process_start: string, hour_appr_process_start: bigint, reg_region_not_live_region: bigint, reg_region_not_work_region: bigint, live_region_not_work_region: bigint, reg_city_not_live_city: bigint, reg_city_not_work_city: bigint, live_city_not_work_city: bigint, organization_type: string, ext_source_1: double, ext_source_2: double, ext_source_3: double, apartments_avg: double, basementarea_avg: double, years_beginexpluatation_avg: double, years_build_avg: double, commonarea_avg: double, elevators_avg: double, entrances_avg: double, floorsmax_avg: double, floorsmin_avg: double, landarea_avg: double, livingapartments_avg: double, livingarea_avg: double, nonlivingapartments_avg: double, nonlivingarea_avg: double, apartments_mode: double, basementarea_mode: double, years_beginexpluatation_mode: double, years_build_mode: double, commonarea_mode: double, elevators_mode: double, entrances_mode: double, floorsmax_mode: double, floorsmin_mode: double, landarea_mode: double, livingapartments_mode: double, livingarea_mode: double, nonlivingapartments_mode: double, nonlivingarea_mode: double, apartments_medi: double, basementarea_medi: double, years_beginexpluatation_medi: double, years_build_medi: double, commonarea_medi: double, elevators_medi: double, entrances_medi: double, floorsmax_medi: double, floorsmin_medi: double, landarea_medi: double, livingapartments_medi: double, livingarea_medi: double, nonlivingapartments_medi: double, nonlivingarea_medi: double, fondkapremont_mode: string, housetype_mode: string, totalarea_mode: double, wallsmaterial_mode: string, emergencystate_mode: string, obs_30_cnt_social_circle: double, def_30_cnt_social_circle: double, obs_60_cnt_social_circle: double, def_60_cnt_social_circle: double, days_last_phone_change: double, flag_document_2: bigint, flag_document_3: bigint, flag_document_4: bigint, flag_document_5: bigint, flag_document_6: bigint, flag_document_7: bigint, flag_document_8: bigint, flag_document_9: bigint, flag_document_10: bigint, flag_document_11: bigint, flag_document_12: bigint, flag_document_13: bigint, flag_document_14: bigint, flag_document_15: bigint, flag_document_16: bigint, flag_document_17: bigint, flag_document_18: bigint, flag_document_19: bigint, flag_document_20: bigint, flag_document_21: bigint, amt_req_credit_bureau_hour: double, amt_req_credit_bureau_day: double, amt_req_credit_bureau_week: double, amt_req_credit_bureau_mon: double, amt_req_credit_bureau_qrt: double, amt_req_credit_bureau_year: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.option(\"failFast\", \"true\").jdbc(\n",
    "    url=jdbc_url, table=\"application_train\", properties=properties\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b6036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df.columns:\n",
    "#     get_column_summary(df=df, column_name=col)\n",
    "#     print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a92f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for column: main_split_id\n",
      "========================================\n",
      "Data Type        : LongType()\n",
      "Unique Values    : 12\n",
      "Null Count       : 0\n",
      "Total Rows       : 307511\n",
      "Null Percentage  : 0.0 %\n",
      "Minimum value: 1\n",
      "Maximum value: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 unique values : [7, 10, 12, 8, 1]\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_index = 0\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa079d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 1\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 2\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba9882",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 3\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80995d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# indexer = StringIndexer(\n",
    "#     inputCol=\"name_contract_type\", outputCol=\"name_contract_type_string_indexed\"\n",
    "# )\n",
    "# model = indexer.fit(df)\n",
    "# df = model.transform(df)\n",
    "\n",
    "# from pyspark.sql import Row\n",
    "\n",
    "# temp_df = spark.createDataFrame(\n",
    "#     [Row(name_contract_type=item) for item in model.labels]\n",
    "# )\n",
    "# temp_df.show()\n",
    "# yo_indexer = StringIndexer(\n",
    "#     inputCol=\"name_contract_type\", outputCol=\"name_contract_type_Index\"\n",
    "# )\n",
    "\n",
    "# yo_model = yo_indexer.fit(temp_df)\n",
    "# yo_model.labels == model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 4\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c1a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 5\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4301e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 6\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c4576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64b10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for col_name in [\n",
    "    # \"name_contract_type\",\n",
    "    # \"code_gender\",\n",
    "    \"flag_own_car\",\n",
    "    \"flag_own_realty\",\n",
    "]:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_string_indexed\")\n",
    "    model = indexer.fit(df)\n",
    "    df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 7\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163395b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 8\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bfc4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 9\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98451b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for column: amt_annuity\n",
      "========================================\n",
      "Data Type        : DoubleType()\n",
      "Unique Values    : 13672\n",
      "Null Count       : 12\n",
      "Total Rows       : 307511\n",
      "Null Percentage  : 0.003902299429939092 %\n",
      "Minimum value: 1615.5\n",
      "Maximum value: 258025.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 unique values : [9000.0, 13500.0, 6750.0, 10125.0, 37800.0]\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_index = 10\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04462f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"amt_credit\"], outputCol=\"amt_annuity_lr_features\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df.filter(col(\"amt_annuity\").isNotNull()))\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"amt_annuity_lr_features\",\n",
    "    labelCol=\"amt_annuity\",\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8,\n",
    ")\n",
    "\n",
    "lrModel = lr.fit(df_assembled)\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "print(\"RMSE: %f\" % lrModel.summary.rootMeanSquaredError)\n",
    "print(\"R2 Score: %f\" % lrModel.summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a659a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, when\n",
    "\n",
    "coefficient = lrModel.coefficients[0]\n",
    "intercept = lrModel.intercept\n",
    "expr_string = f\"({coefficient} * amt_credit) + {intercept}\"\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"amt_annuity\",\n",
    "    when(col(\"amt_annuity\").isNull(), expr(expr_string)).otherwise(col(\"amt_annuity\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd0339a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for column: amt_goods_price\n",
      "========================================\n",
      "Data Type        : DoubleType()\n",
      "Unique Values    : 1002\n",
      "Null Count       : 278\n",
      "Total Rows       : 307511\n",
      "Null Percentage  : 0.09040327012692229 %\n",
      "Minimum value: 40500.0\n",
      "Maximum value: 4050000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 unique values : [450000.0, 225000.0, 675000.0, 900000.0, 270000.0]\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "col_index = 11\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cb4f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"amt_credit\"], outputCol=\"amt_goods_price_lr_features\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df.filter(col(\"amt_goods_price\").isNotNull()))\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol=\"amt_goods_price_lr_features\",\n",
    "    labelCol=\"amt_goods_price\",\n",
    "    regParam=0.3,\n",
    "    elasticNetParam=0.8,\n",
    ")\n",
    "\n",
    "lrModel = lr.fit(df_assembled)\n",
    "\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "print(\"RMSE: %f\" % lrModel.summary.rootMeanSquaredError)\n",
    "print(\"R2 Score: %f\" % lrModel.summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d84b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr, when\n",
    "\n",
    "coefficient = lrModel.coefficients[0]\n",
    "intercept = lrModel.intercept\n",
    "expr_string = f\"({coefficient} * amt_credit) + {intercept}\"\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"amt_goods_price\",\n",
    "    when(col(\"amt_goods_price\").isNull(), expr(expr_string)).otherwise(col(\"amt_goods_price\")),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471da54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_index = 12\n",
    "get_column_summary(df=df, column_name=df.columns[col_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69199b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727f9b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5f9a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
